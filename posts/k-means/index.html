<!doctype html><html lang=en-us><head><link rel=preload href=/lib/font-awesome/webfonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><script type=text/javascript src=https://latest.cactus.chat/cactus.js></script>
<link rel=stylesheet href=https://latest.cactus.chat/style.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>K-means in Python | mrr1vfe</title><link rel=canonical href=https://www.reidy.icu/posts/k-means/><meta name=description content="This is my website"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="K-means in Python"><meta property="og:description" content="K-means There are two major steps in the K-means algorithm. The first one is to calculate the representatives (centroids) of a given partition. The second one is to find the partition based on the representatives.
Inputs Suppose we have a dataset looks like this:
1dataset = np.array([[5, 6], 2 [6, 5], 3 [0, 1], 4 [1, 0], 5 [3, 3]]) Each row in this dataset matrix is an observation and each column in this matrix represents a feature."><meta property="og:type" content="article"><meta property="og:url" content="https://www.reidy.icu/posts/k-means/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-19T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-19T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="K-means in Python"><meta name=twitter:description content="K-means There are two major steps in the K-means algorithm. The first one is to calculate the representatives (centroids) of a given partition. The second one is to find the partition based on the representatives.
Inputs Suppose we have a dataset looks like this:
1dataset = np.array([[5, 6], 2 [6, 5], 3 [0, 1], 4 [1, 0], 5 [3, 3]]) Each row in this dataset matrix is an observation and each column in this matrix represents a feature."><link rel=stylesheet href=https://www.reidy.icu/css/styles.4c2b9aa1d874d6766f554b2d404e8fd62ab4761f51ee9b3f358d12e81e7fa43a1b4378db995bc1926bbe5ed98c060be5e7bd4f2470504cf94f22b4b3a74e62b6.css integrity="sha512-TCuaodh01nZvVUstQE6P1iq0dh9R7ps/NY0S6B5/pDobQ3jbmVvBkmu+XtmMBgvl571PJHBQTPlPIrSzp05itg=="><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://www.reidy.icu/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://www.reidy.icu/about/ aria-label=Next><i class="fas fa-chevron-right" aria-hidden=true onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up" aria-hidden=true onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class=icon href=# aria-label=Share><i class="fas fa-share-alt" aria-hidden=true onmouseover='$("#i-share").toggle()' onmouseout='$("#i-share").toggle()' onclick='return $("#share").toggle(),!1'></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f" aria-label=Facebook><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&text=K-means%20in%20Python" aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&title=K-means%20in%20Python" aria-label=Linkedin><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&is_video=false&description=K-means%20in%20Python" aria-label=Pinterest><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=K-means%20in%20Python&body=Check out this article: https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f" aria-label=Email><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&title=K-means%20in%20Python" aria-label=Pocket><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&title=K-means%20in%20Python" aria-label=reddit><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&name=K-means%20in%20Python&description=K-means%20There%20are%20two%20major%20steps%20in%20the%20K-means%20algorithm.%20The%20first%20one%20is%20to%20calculate%20the%20representatives%20%28centroids%29%20of%20a%20given%20partition.%20The%20second%20one%20is%20to%20find%20the%20partition%20based%20on%20the%20representatives.%0aInputs%20Suppose%20we%20have%20a%20dataset%20looks%20like%20this%3a%0a1dataset%20%3d%20np.array%28%5b%5b5%2c%206%5d%2c%202%20%5b6%2c%205%5d%2c%203%20%5b0%2c%201%5d%2c%204%20%5b1%2c%200%5d%2c%205%20%5b3%2c%203%5d%5d%29%20Each%20row%20in%20this%20dataset%20matrix%20is%20an%20observation%20and%20each%20column%20in%20this%20matrix%20represents%20a%20feature." aria-label=Tumblr><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&t=K-means%20in%20Python" aria-label="Hacker News"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#inputs>Inputs</a></li><li><a href=#finding-the-centroids>Finding the centroids</a></li><li><a href=#finding-the-partition>Finding the partition</a></li><li><a href=#putting-everything-together>Putting everything together</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">K-means in Python</h1><div class=meta><div class=postdate><time datetime="2021-10-19 00:00:00 +0000 UTC" itemprop=datePublished>2021-10-19</time></div><div class=article-read-time><i class="far fa-clock"></i>
3 minute read</div><div class=article-tag><i class="fas fa-tag"></i>
<a class=tag-link href=/tags/ml rel=tag>ML</a></div></div></header><div class=content itemprop=articleBody><h1 id=k-means>K-means</h1><p>There are two major steps in the K-means algorithm. The first one is to calculate the representatives (centroids) of a given partition. The second one is to find the partition based on the representatives.</p><h2 id=inputs>Inputs</h2><p>Suppose we have a dataset looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span>dataset <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([[<span style=color:#bd93f9>5</span>, <span style=color:#bd93f9>6</span>],
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span><span>                    [<span style=color:#bd93f9>6</span>, <span style=color:#bd93f9>5</span>],
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3</span><span>                    [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>],
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4</span><span>                    [<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>],
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5</span><span>                    [<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>3</span>]])
</span></span></code></pre></div><p>Each row in this dataset matrix is an observation and each column in this matrix represents a feature. So, in this example, we have 5 points from a plane.
And we define partition in the following way:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span>partition <span style=color:#ff79c6>=</span> [[<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>4</span>], [<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>2</span>]]
</span></span></code></pre></div><p>Observe that <code>partition</code> has a length of 2, which implies that the $k$ for the K-means algorithm is 2. Each list in <code>partition</code> represents a cluster. Elements within each list is the corresponding index of that observation in the dataset. So, with respect to this <code>partition</code>, the first cluster has 3 elements, namely <code>[5, 6], [1, 0], [3, 3]</code>, and the second cluster contains <code>[6, 5]</code> and <code>[0, 1]</code>.</p><h2 id=finding-the-centroids>Finding the centroids</h2><p>To calculate the centroids, we need information about the dataset and about the partition. The centroid for a cluster $C$ is the average of all observations in the cluster, namely
$$ \mu = \frac{1}{|C|}\sum_{i \in C} x_i $$,
where $x_i$ is the $i^\text{th}$ observation in the dataset.</p><p>All we need to do is to calculate the mean (using <code>np.mean()</code> with <code>axis</code> set to 0) for each partition.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>find_centroids</span>(dataset: np<span style=color:#ff79c6>.</span>ndarray, partition: <span style=color:#8be9fd;font-style:italic>list</span>) <span style=color:#ff79c6>-&gt;</span> np<span style=color:#ff79c6>.</span>ndarray:
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span><span><span style=color:#f1fa8c>    find the centroids of the given partition
</span></span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span><span>    u <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span><span>    <span style=color:#ff79c6>for</span> indices <span style=color:#ff79c6>in</span> partition:
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span><span>        <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(indices) <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span><span>            u<span style=color:#ff79c6>.</span>append(np<span style=color:#ff79c6>.</span>zeros((dataset<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>1</span>], )))
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span><span>        <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span><span>            u<span style=color:#ff79c6>.</span>append(np<span style=color:#ff79c6>.</span>mean(dataset[indices], axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>))
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span><span>    <span style=color:#ff79c6>return</span> np<span style=color:#ff79c6>.</span>array(u)
</span></span></code></pre></div><p>Each centroid is stored in the list $u$ (which is returned as an numpy array). Note that the length of $u$ should be equivalent to $k$.</p><h2 id=finding-the-partition>Finding the partition</h2><p>With the centroids calculated, we need to re-partition the dataset based on these (newly calculated) centroids. This process is observation-wise, which means for each observation in the dataset, we need to compare it to each of the centroids, which represent clusters in a one-to-one manner, and find to which centroid the observation is close. The observation will be assigned to the cluster that represented by the centroid.</p><p>We could use the Euclidean distance, i.e. <code>np.linalg.norm()</code>, to find the distance between an obervation to all centroids at the same time. Then, use the <code>np.argmin()</code> function to select the index of the least distance. This index will be the cluster index for this observation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>new_partition</span>(dataset: np<span style=color:#ff79c6>.</span>ndarray, u: np<span style=color:#ff79c6>.</span>ndarray) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>list</span>:
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3</span><span><span style=color:#f1fa8c>    find the new partition based on u
</span></span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4</span><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5</span><span>    partition <span style=color:#ff79c6>=</span> [[] <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(u<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>])]
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6</span><span>    <span style=color:#ff79c6>for</span> i, point <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(dataset):
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7</span><span>        argmin <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>argmin(np<span style=color:#ff79c6>.</span>linalg<span style=color:#ff79c6>.</span>norm(point <span style=color:#ff79c6>-</span> u, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8</span><span>        partition[argmin]<span style=color:#ff79c6>.</span>append(i)
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">9</span><span>    <span style=color:#ff79c6>return</span> partition
</span></span></code></pre></div><h2 id=putting-everything-together>Putting everything together</h2><p>K-means is an iterative algorithm. All we need to do is to put the first step and the second step in a for loop. Here, we hard code the epoch number to 3. And we hard code the initial partition. One, instead, can randomize the initial partition and run the algorithm to see which convergence has the best result. And one can check whether the loss of a new partition is improved significantly or not to determine if the algorithm should halt or not.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>kmeans</span>(dataset: np<span style=color:#ff79c6>.</span>ndarray, partition: <span style=color:#8be9fd;font-style:italic>list</span>):
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span><span><span style=color:#f1fa8c>    K-means algorithm, k can be inferred from the shape of partition
</span></span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span><span>    epoch <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span><span>    losses <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#34;epoch&#34;</span>: [], <span style=color:#f1fa8c>&#34;loss&#34;</span>: []}
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span><span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span><span>    <span style=color:#ff79c6>while</span> epoch <span style=color:#ff79c6>&lt;</span> <span style=color:#bd93f9>3</span>:
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span><span>        epoch <span style=color:#ff79c6>+=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span><span>
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span><span>        u <span style=color:#ff79c6>=</span> find_centroids(dataset, partition)
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12</span><span>        partition <span style=color:#ff79c6>=</span> new_partition(dataset, u)
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13</span><span>    
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14</span><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n\n</span><span style=color:#f1fa8c>Final Centroids:&#34;</span>)
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15</span><span>    <span style=color:#8be9fd;font-style:italic>print</span>(u)
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16</span><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Final Partition:&#34;</span>)
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17</span><span>    <span style=color:#ff79c6>for</span> i, indices <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(partition):
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18</span><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\t</span><span style=color:#f1fa8c>cluster #</span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>:&#34;</span><span style=color:#ff79c6>.</span>format(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>), {<span style=color:#8be9fd;font-style:italic>tuple</span>(dataset[index])
</span></span><span style=display:flex><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19</span><span>                                               <span style=color:#ff79c6>for</span> index <span style=color:#ff79c6>in</span> indices})
</span></span></code></pre></div></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#inputs>Inputs</a></li><li><a href=#finding-the-centroids>Finding the centroids</a></li><li><a href=#finding-the-partition>Finding the partition</a></li><li><a href=#putting-everything-together>Putting everything together</a></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f" aria-label=Facebook><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&text=K-means%20in%20Python" aria-label=Twitter><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&title=K-means%20in%20Python" aria-label=Linkedin><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&is_video=false&description=K-means%20in%20Python" aria-label=Pinterest><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=K-means%20in%20Python&body=Check out this article: https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f" aria-label=Email><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&title=K-means%20in%20Python" aria-label=Pocket><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&title=K-means%20in%20Python" aria-label=reddit><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&name=K-means%20in%20Python&description=K-means%20There%20are%20two%20major%20steps%20in%20the%20K-means%20algorithm.%20The%20first%20one%20is%20to%20calculate%20the%20representatives%20%28centroids%29%20of%20a%20given%20partition.%20The%20second%20one%20is%20to%20find%20the%20partition%20based%20on%20the%20representatives.%0aInputs%20Suppose%20we%20have%20a%20dataset%20looks%20like%20this%3a%0a1dataset%20%3d%20np.array%28%5b%5b5%2c%206%5d%2c%202%20%5b6%2c%205%5d%2c%203%20%5b0%2c%201%5d%2c%204%20%5b1%2c%200%5d%2c%205%20%5b3%2c%203%5d%5d%29%20Each%20row%20in%20this%20dataset%20matrix%20is%20an%20observation%20and%20each%20column%20in%20this%20matrix%20represents%20a%20feature." aria-label=Tumblr><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fk-means%2f&t=K-means%20in%20Python" aria-label="Hacker News"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2022 Reid Chen</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
<script src=/js/code-copy.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>