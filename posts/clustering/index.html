<!doctype html><html lang=en-us><head><link rel=preload href=/lib/font-awesome/webfonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><script type=text/javascript src=https://latest.cactus.chat/cactus.js></script>
<link rel=stylesheet href=https://latest.cactus.chat/style.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Clustering | mrr1vfe</title><link rel=canonical href=https://www.reidy.icu/posts/clustering/><meta name=description content="This is my website"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Clustering"><meta property="og:description" content="In unsupervised learning, there are no labels associated with features. Generally speaking, the ultimate goal of unsupervised learning is to find patterns and structures that help us to better understand data. Sometimes, we also use unsupervised learning to model a distribution. But we generally will not make predictions.
There are 3 types of clustering
Partitional (centroid, graph-theoretic, spectral) Hierarchical (agglomerative, divisive) Bayesian (decision-based, non-parametric) Partitional Clustering $k$-means $k$-means is a type of partitional centroid-based clustering algorithm."><meta property="og:type" content="article"><meta property="og:url" content="https://www.reidy.icu/posts/clustering/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-08T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-08T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Clustering"><meta name=twitter:description content="In unsupervised learning, there are no labels associated with features. Generally speaking, the ultimate goal of unsupervised learning is to find patterns and structures that help us to better understand data. Sometimes, we also use unsupervised learning to model a distribution. But we generally will not make predictions.
There are 3 types of clustering
Partitional (centroid, graph-theoretic, spectral) Hierarchical (agglomerative, divisive) Bayesian (decision-based, non-parametric) Partitional Clustering $k$-means $k$-means is a type of partitional centroid-based clustering algorithm."><link rel=stylesheet href=https://www.reidy.icu/css/styles.4c2b9aa1d874d6766f554b2d404e8fd62ab4761f51ee9b3f358d12e81e7fa43a1b4378db995bc1926bbe5ed98c060be5e7bd4f2470504cf94f22b4b3a74e62b6.css integrity="sha512-TCuaodh01nZvVUstQE6P1iq0dh9R7ps/NY0S6B5/pDobQ3jbmVvBkmu+XtmMBgvl571PJHBQTPlPIrSzp05itg=="><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://www.reidy.icu/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://www.reidy.icu/about/ aria-label=Previous><i class="fas fa-chevron-left" aria-hidden=true onmouseover='$("#i-prev").toggle()' onmouseout='$("#i-prev").toggle()'></i></a></li><li><a class=icon href=https://www.reidy.icu/posts/perceptron-learning-algorithm/ aria-label=Next><i class="fas fa-chevron-right" aria-hidden=true onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up" aria-hidden=true onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class=icon href=# aria-label=Share><i class="fas fa-share-alt" aria-hidden=true onmouseover='$("#i-share").toggle()' onmouseout='$("#i-share").toggle()' onclick='return $("#share").toggle(),!1'></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f" aria-label=Facebook><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&text=Clustering" aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&title=Clustering" aria-label=Linkedin><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&is_video=false&description=Clustering" aria-label=Pinterest><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Clustering&body=Check out this article: https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f" aria-label=Email><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&title=Clustering" aria-label=Pocket><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&title=Clustering" aria-label=reddit><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&name=Clustering&description=In%20unsupervised%20learning%2c%20there%20are%20no%20labels%20associated%20with%20features.%20Generally%20speaking%2c%20the%20ultimate%20goal%20of%20unsupervised%20learning%20is%20to%20find%20patterns%20and%20structures%20that%20help%20us%20to%20better%20understand%20data.%20Sometimes%2c%20we%20also%20use%20unsupervised%20learning%20to%20model%20a%20distribution.%20But%20we%20generally%20will%20not%20make%20predictions.%0aThere%20are%203%20types%20of%20clustering%0aPartitional%20%28centroid%2c%20graph-theoretic%2c%20spectral%29%20Hierarchical%20%28agglomerative%2c%20divisive%29%20Bayesian%20%28decision-based%2c%20non-parametric%29%20Partitional%20Clustering%20%24k%24-means%20%24k%24-means%20is%20a%20type%20of%20partitional%20centroid-based%20clustering%20algorithm." aria-label=Tumblr><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&t=Clustering" aria-label="Hacker News"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#partitional-clustering>Partitional Clustering</a><ul><li><a href=#k-means>$k$-means</a></li><li><a href=#graph-based>Graph-based</a><ul><li><a href=#spectral-clustering>Spectral Clustering</a></li></ul></li></ul></li><li><a href=#hierarchical-clustering>Hierarchical Clustering</a><ul><li><a href=#agglomerative-clustering>Agglomerative Clustering</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Clustering</h1><div class=meta><div class=postdate><time datetime="2022-10-08 00:00:00 +0000 UTC" itemprop=datePublished>2022-10-08</time></div><div class=article-read-time><i class="far fa-clock"></i>
3 minute read</div><div class=article-tag><i class="fas fa-tag"></i>
<a class=tag-link href=/tags/ml rel=tag>ml</a></div></div></header><div class=content itemprop=articleBody><p>In unsupervised learning, there are no labels associated with features. Generally speaking, the ultimate goal of unsupervised learning is to find patterns and structures that help us to better understand data. Sometimes, we also use unsupervised learning to model a distribution. But we generally will not make predictions.</p><p>There are 3 types of clustering</p><ol><li>Partitional (centroid, graph-theoretic, spectral)</li><li>Hierarchical (agglomerative, divisive)</li><li>Bayesian (decision-based, non-parametric)</li></ol><h2 id=partitional-clustering>Partitional Clustering</h2><h3 id=k-means>$k$-means</h3><p>$k$-means is a type of partitional centroid-based clustering algorithm. The algorithm is described as follows:</p><ol><li>Randomly pick $k$ cluster centers;</li><li>Find the closest center for each point;</li><li>Update cluster centers by computing centroids;</li><li>While not converging, jump to step 2.</li></ol><h3 id=graph-based>Graph-based</h3><p>Let $G = (V, E)$ has vertex set $V$ and edge set $E$. Each $e \in E$ can be weighted or unweighted, and it encodes the similarity between data points.</p><p>If each vertex represents a data point, then finding a clustering amongst these points is isomorphic to partition $V$ into $V_1$ and $V_2$ (when $k = 2$). The partition of $V$ implies that we need to split the graph. We can define an objective function to determine the best way to split the edges of a graph. Then, we can optimize the objective function in order to find the optimal partition. Consider the objective function to be $\text{Cut}(V_1, V_2) = \sum_{i \in V_1, j \in V_2} w_{ij}$, then we would like to have split $V$ so that the $\text{Cut}$ is minimized. Of course, such a greedy approach could lead to a less ideal solution: $|V_1| &#171; |V_2| (|V_2| = |V| - 1)$. We want to balance the cardinality of $V_1$ and $V_2$. A way to balance it is to use &ldquo;balanced&rdquo; cut like:
$$
\text{Ratio Cut}(V_1, V_2) = \frac{\text{Cut}(V_1, V_2)}{|V_1|} + \frac{\text{Cut}(V_1, V_2)}{|V_2|},
$$
or
$$
\text{Normalized Cut}(V_1, V_2) = \frac{\text{Cut}(V_1, V_2)}{\sum_{i \in V_1} d_i} + \frac{\text{Cut}(V_1, V_2)}{\sum_{j \in V_2} d_j},
$$
where $d_i = \sum_j w_{ij}$.</p><h4 id=spectral-clustering>Spectral Clustering</h4><p>We start with a similarity/adjacency matrix, $A$, of a graph $G$. Let $D$ be diagonal matrix $D$ such that the i-$th$ diagonal entry is $\sum^n_{k=1} w_{ik}$. Define graph Laplacian matrix $L = D - A$. $L$ has the 2 following properties:</p><ol><li>L is symmetric</li><li>L is positive semi-definite
The second properties implies that all eigenvalues of $L$ are non-negative. Then, compute the $k$ smallest eigenvectors and stack them as columns into a matrix $V$. Finally, we run $k$-means on the rows of $V$ to obtain the clustering result.</li></ol><h2 id=hierarchical-clustering>Hierarchical Clustering</h2><p>The basic idea of hierarchical clustering is to build hierarchy amongst the data points, i.e. to form an arrangement of these points from specific to general. The advantage of such an algorithm is that there is no need for $k$, the number of clusters. The output of this algorithm is a binary tree. There are two types of hierarchical clustering, which are described as follows:</p><ol><li>Agglomerative clustering: A buttom-up approach, which initially treats each data point as its own singleton cluster and progressively merge clusters.</li><li>Divisive: A top-down approach, which initially treats all points as in a single cluster and progressively split clusters.</li></ol><h3 id=agglomerative-clustering>Agglomerative Clustering</h3><ol><li>Every point is in its own cluster;</li><li>For all pair of clusters, select the closest pair and merge them;</li><li>Repeat step 2 until there is only 1 cluster left.</li></ol><p>In step 2, we need to calculate the distance between all pairs of clusters in order to select the closest one to merge. There are 3 ways to define the distance between two clusters $A$ and $B$:</p><ol><li>single-linkage: $d(A, B) = \min_{x_1 \in A, x_2 \in B} d(x_1, x_2)$;</li><li>complete-linkage: $d(A, B) = \max_{x_1 \in A, x_2 \in B} d(x_1, x_2)$;</li><li>average-linkage: $d(A, B) = \frac{1}{|A| |B|}\sum_{x_1 \in A, x_2 \in B} d(x_1 , x_2)$.</li></ol></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#partitional-clustering>Partitional Clustering</a><ul><li><a href=#k-means>$k$-means</a></li><li><a href=#graph-based>Graph-based</a><ul><li><a href=#spectral-clustering>Spectral Clustering</a></li></ul></li></ul></li><li><a href=#hierarchical-clustering>Hierarchical Clustering</a><ul><li><a href=#agglomerative-clustering>Agglomerative Clustering</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f" aria-label=Facebook><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&text=Clustering" aria-label=Twitter><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&title=Clustering" aria-label=Linkedin><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&is_video=false&description=Clustering" aria-label=Pinterest><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Clustering&body=Check out this article: https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f" aria-label=Email><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&title=Clustering" aria-label=Pocket><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&title=Clustering" aria-label=reddit><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&name=Clustering&description=In%20unsupervised%20learning%2c%20there%20are%20no%20labels%20associated%20with%20features.%20Generally%20speaking%2c%20the%20ultimate%20goal%20of%20unsupervised%20learning%20is%20to%20find%20patterns%20and%20structures%20that%20help%20us%20to%20better%20understand%20data.%20Sometimes%2c%20we%20also%20use%20unsupervised%20learning%20to%20model%20a%20distribution.%20But%20we%20generally%20will%20not%20make%20predictions.%0aThere%20are%203%20types%20of%20clustering%0aPartitional%20%28centroid%2c%20graph-theoretic%2c%20spectral%29%20Hierarchical%20%28agglomerative%2c%20divisive%29%20Bayesian%20%28decision-based%2c%20non-parametric%29%20Partitional%20Clustering%20%24k%24-means%20%24k%24-means%20is%20a%20type%20of%20partitional%20centroid-based%20clustering%20algorithm." aria-label=Tumblr><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fclustering%2f&t=Clustering" aria-label="Hacker News"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2022 Reid Chen</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
<script src=/js/code-copy.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>