<!doctype html><html lang=en-us><head><link rel=preload href=/lib/font-awesome/webfonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><script type=text/javascript src=https://latest.cactus.chat/cactus.js></script>
<link rel=stylesheet href=https://latest.cactus.chat/style.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Perceptron Learning Algorithm | mrr1vfe</title><link rel=canonical href=https://www.reidy.icu/posts/perceptron-learning-algorithm/><meta name=description content="This is my website"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Perceptron Learning Algorithm"><meta property="og:description" content="Given a dataset $\mathcal{D} = {(\vec{x}_1, y_1), \cdots, (\vec{x}_N, y_N)}$ and a hypothesis set $\mathcal{H}$, our learning algorithm $\mathcal{A}$ tries to learn a function $g \in \mathcal{H}$ that approximates the underlying, true function $f: \mathcal{X} \to \mathcal{Y}$, which generates the points in $\mathcal{D}$.
Credit Card Approve Problem Given a customer who is applying for a credit card, we want to build a system that determines if we should grant the application or not based on the customer&rsquo;s information such as age, annual salary, year in job, etc."><meta property="og:type" content="article"><meta property="og:url" content="https://www.reidy.icu/posts/perceptron-learning-algorithm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-30T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Perceptron Learning Algorithm"><meta name=twitter:description content="Given a dataset $\mathcal{D} = {(\vec{x}_1, y_1), \cdots, (\vec{x}_N, y_N)}$ and a hypothesis set $\mathcal{H}$, our learning algorithm $\mathcal{A}$ tries to learn a function $g \in \mathcal{H}$ that approximates the underlying, true function $f: \mathcal{X} \to \mathcal{Y}$, which generates the points in $\mathcal{D}$.
Credit Card Approve Problem Given a customer who is applying for a credit card, we want to build a system that determines if we should grant the application or not based on the customer&rsquo;s information such as age, annual salary, year in job, etc."><link rel=stylesheet href=https://www.reidy.icu/css/styles.4c2b9aa1d874d6766f554b2d404e8fd62ab4761f51ee9b3f358d12e81e7fa43a1b4378db995bc1926bbe5ed98c060be5e7bd4f2470504cf94f22b4b3a74e62b6.css integrity="sha512-TCuaodh01nZvVUstQE6P1iq0dh9R7ps/NY0S6B5/pDobQ3jbmVvBkmu+XtmMBgvl571PJHBQTPlPIrSzp05itg=="><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://www.reidy.icu/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://www.reidy.icu/about/ aria-label=Previous><i class="fas fa-chevron-left" aria-hidden=true onmouseover='$("#i-prev").toggle()' onmouseout='$("#i-prev").toggle()'></i></a></li><li><a class=icon href=https://www.reidy.icu/posts/lemmas/ aria-label=Next><i class="fas fa-chevron-right" aria-hidden=true onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up" aria-hidden=true onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class=icon href=# aria-label=Share><i class="fas fa-share-alt" aria-hidden=true onmouseover='$("#i-share").toggle()' onmouseout='$("#i-share").toggle()' onclick='return $("#share").toggle(),!1'></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f" aria-label=Facebook><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&text=Perceptron%20Learning%20Algorithm" aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&title=Perceptron%20Learning%20Algorithm" aria-label=Linkedin><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&is_video=false&description=Perceptron%20Learning%20Algorithm" aria-label=Pinterest><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Perceptron%20Learning%20Algorithm&body=Check out this article: https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f" aria-label=Email><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&title=Perceptron%20Learning%20Algorithm" aria-label=Pocket><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&title=Perceptron%20Learning%20Algorithm" aria-label=reddit><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&name=Perceptron%20Learning%20Algorithm&description=Given%20a%20dataset%20%24%5cmathcal%7bD%7d%20%3d%20%7b%28%5cvec%7bx%7d_1%2c%20y_1%29%2c%20%5ccdots%2c%20%28%5cvec%7bx%7d_N%2c%20y_N%29%7d%24%20and%20a%20hypothesis%20set%20%24%5cmathcal%7bH%7d%24%2c%20our%20learning%20algorithm%20%24%5cmathcal%7bA%7d%24%20tries%20to%20learn%20a%20function%20%24g%20%5cin%20%5cmathcal%7bH%7d%24%20that%20approximates%20the%20underlying%2c%20true%20function%20%24f%3a%20%5cmathcal%7bX%7d%20%5cto%20%5cmathcal%7bY%7d%24%2c%20which%20generates%20the%20points%20in%20%24%5cmathcal%7bD%7d%24.%0aCredit%20Card%20Approve%20Problem%20Given%20a%20customer%20who%20is%20applying%20for%20a%20credit%20card%2c%20we%20want%20to%20build%20a%20system%20that%20determines%20if%20we%20should%20grant%20the%20application%20or%20not%20based%20on%20the%20customer%26rsquo%3bs%20information%20such%20as%20age%2c%20annual%20salary%2c%20year%20in%20job%2c%20etc." aria-label=Tumblr><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&t=Perceptron%20Learning%20Algorithm" aria-label="Hacker News"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#credit-card-approve-problem>Credit Card Approve Problem</a></li><li><a href=#perceptron-hypothesis>Perceptron Hypothesis</a><ul><li><a href=#perceptron-in-mathbbr2>Perceptron in $\mathbb{R}^2$</a></li></ul></li><li><a href=#perceptron-learning-algorithm>Perceptron Learning Algorithm</a></li><li><a href=#linear-separability>Linear Separability</a></li><li><a href=#pros-and-cons-of-pla>Pros and Cons of PLA</a></li><li><a href=#learning-with-noisy-data>Learning with Noisy Data</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Perceptron Learning Algorithm</h1><div class=meta><div class=postdate><time datetime="2022-11-30 00:00:00 +0000 UTC" itemprop=datePublished>2022-11-30</time></div><div class=article-read-time><i class="far fa-clock"></i>
6 minute read</div><div class=article-tag><i class="fas fa-tag"></i>
<a class=tag-link href=/tags/ml rel=tag>ml</a></div></div></header><div class=content itemprop=articleBody><p>Given a dataset $\mathcal{D} = {(\vec{x}_1, y_1), \cdots, (\vec{x}_N, y_N)}$ and a hypothesis set $\mathcal{H}$, our learning algorithm $\mathcal{A}$ tries to learn a function $g \in \mathcal{H}$ that approximates the underlying, true function $f: \mathcal{X} \to \mathcal{Y}$, which generates the points in $\mathcal{D}$.</p><h2 id=credit-card-approve-problem>Credit Card Approve Problem</h2><p>Given a customer who is applying for a credit card, we want to build a system that determines if we should grant the application or not based on the customer&rsquo;s information such as age, annual salary, year in job, etc. The bank&rsquo;s historical credit approval data can be seen as a dataset $\mathcal{D} = {(\vec{x}<em>i, y_i)}</em>{i=1}^N$where each $\vx_i \in \mathcal{X}$ and each represents a customer. There is a target function $f: \mathcal{X} \to \mathcal{Y}$ that gives $\vx$&rsquo;s credit behavior $f(\vec{x}) = y$. Each $\vx$ is a multidimensional vector where each component is a feature (age, for example). Our learning algorithm $\mathcal{A}$ considers a hypothesis class $\mathcal{H}$ and takes the dataset $\mathcal{D}$ and tries to give a function $g \in \mathcal{H}$ so that $g$ performs similar to $f$. We will use $g$ as our system of approving credit card.</p><h2 id=perceptron-hypothesis>Perceptron Hypothesis</h2><p>Let $\vec{x} = (x_1, x_2, \cdots, x_d)$ be a customer, we compute a weighted score $\sum^d_{i=1} w_ix_i$ such that if the weighted score is greater than a threshold $t$, we approve the credit card, otherwise we don&rsquo;t. Furthermore, suppose $\mathcal{Y} = {+1, -1}$ where $+1$ means good credit and $-1$ means bad credit, then $h \in \mathcal{H}$ looks like
$$
h(\vec{x}) = \sign\left( \left( \sum^d_{i=1} w_ix_i \right) - t\right)
$$
The $h$ here depends on, or is parametrized by ${w_i}$ and $t$. We call the set of all possible $h$ (the set of all possible combination of ${w_i}$ and t) is called perceptron.</p><p>Observe that we can move $t$ into the summation by extending $w_i$ and $x_i$ so that $w_0 = -t$ and $x_0 = 1$. Then, we can write the summation as an inner product:
$$
h(\vx) = \sign(\vec{w}^T\vx).
$$
Essentially, this $\vec{w}$ represents a hypothesis. We now assume that $\vec{w}$ always includes the (negative) threshold $t$.</p><h3 id=perceptron-in-mathbbr2>Perceptron in $\mathbb{R}^2$</h3><p>In $\mathbb{R}^2$, $\vx = [x_1 \quad x_2]^T$, then, the hypothesis is
$$
h(\vx) = \sign(w_0 + w_1x_1 + w_2x_2.)
$$
We can plot all $\vx$ in the dataset in a graph. Moreover, we use $\circ$ to represent $+1$ and $\times$ to represent $-1$. Besides, observe that $w_0 + w_1x_1 + w_2x_2$ is a line. $h(\vx) = +1$ if $\vx$ lies on one side of the line, and $-1$ otherwise on the other side. Since the perceptron is essentially using a line to make a decision, we also call such a hypothesis class linear binary classifier.
![[IMG_D700A668D503-1.jpeg]]</p><h2 id=perceptron-learning-algorithm>Perceptron Learning Algorithm</h2><p>With $\mathcal{H}$, all possible perceptron, how to we select the best of them, denoted as $g$, such that $g$ is close to $f$, which we never know. How do we decide which $h \in \mathcal{H}$ should be our $g$, the best?
Since $\mathcal{D}$ is generated by $f$, we ideally want
$$
g(\vx_i) = f(\vx_i) = y_i.
$$
We can focus on finding the $g$ that has this property. However, it is difficult, since each $h$ in $\mathcal{H}$ is parametrized by a line, and there are infinitely (uncountable) many candidates for us to choose.</p><p>We can start from some $g_0$, and iteratively corrects its mistakes on $\mathcal{D}$. We use $\vec{w}_0$ represents $g_0$ and say $\vec{w}_0 = \vec{0}$.</p><hr><p>Algorithm (Perceptron Learning Algorithm):
For $t = 0, 1, \ldots$</p><ol><li>Find a mistake of $\vec{w}<em>t$, called $(\vx</em>{n(t)}, y_{n(t)})$. By mistake, we mean that $\sign(\vec{w}^T_t\vx_{n(t)}) \ne y_{n(t)}$.</li><li>We try to correct the mistake by $\vec{w}<em>{t + 1} \leftarrow \vec{w}<em>t + y</em>{n(t)}\vx</em>{n(t)}$.
Until no more mistakes
Return the last $\vec{w}$, called $\vec{w}_{PLA}$ as $g$.</li></ol><hr><p>Algorithm (Cyclic PLA):
For $t = 0, 1, \ldots$</p><ol><li>Find a mistake of $\vec{w}<em>t$, called $(\vx</em>{n(t)}, y_{n(t)})$. By mistake, we mean that $\sign(\vec{w}^T_t\vx_{n(t)}) \ne y_{n(t)}$.</li><li>We try to correct the mistake by $\vec{w}<em>{t + 1} \leftarrow \vec{w}<em>t + y</em>{n(t)}\vx</em>{n(t)}$.
Until a full cycle (of $\mathcal{D}$) not encountering mistakes.
Return the last $\vec{w}$, called $\vec{w}_{PLA}$ as $g$.</li></ol><hr><p>Suppose there is a mistake at iteration $t$, with instance $\vx_n$. We observe that
$$
y_n\vec{w}^T_{t +1} \vx_n \ge y_n \vec{w}^T_t\vx_n
$$
is always true. This shows that the algorithm is indeed trying to correct the mistake. Why? Since it&rsquo;s a mistake, $\sign(\vec{w}<em>t^T \vx_n) \ne y_n$. Therefore, $y_n \vec{w}^T_t\vx_n$ is negative. $y_n\vec{w}^T</em>{t +1} \vx_n$ is greater than that means that it is trying to be positive, which is correct.</p><p>We ponder, if the algorithm halts. If it halts, how does $g$ perform on the instances outside $\mathcal{D}$?</p><h2 id=linear-separability>Linear Separability</h2><p>If PLA halts, i.e. no more mistakes, then $\mathcal{D}$ allows some $\vw$ to make no mistakes. We call such $\mathcal{D}$ linear separable. But given a linear separable $\mathcal{D}$, will PLA halt?</p><p>$\mathcal{D}$ is linear separable, is equivalent to there is an omniscient $\vw_f$ such that $y_n = \sign(\vw^T_f \vx_n)$, for all $n$. In other word,
$$
\min_n y_n \vw^T_f \vx_n > 0.
$$
So, for any $\vx_i$,
$$
y_i\vw^T_f\vx_i \ge \min_ny_n\vw^T_f\vx_n > 0.
$$
To measure the similarity between $\vx_f$ and $\vx_T$, learned at step $T$, we can use the inner product. The larger the inner product, the closer the two vectors are. The following derivation shows that we the algorithm runs from one iteration to the next, our learned line is getting closer and closer to the true one.
$$
\begin{align*}
\vw_f\vw_T &= \vw_f(\vw_{T-1} + y_{n(t)}\vx_{n(t)})\
&= \vw_f\vx_{T-1} + y_{n(t)}\vw_f\vx_{n(t)} \
&> \vw_f\vw_{T-1}
\end{align*}
$$
If you are careful enough, you would say that the magnitude of inner product also depends on the magnitude of the vector. This is true, we would want $\vec{w}<em>T$ to not grow too large as $T$ goes large.
$$
\begin{align*}
\norm{\vw_T}^2 &= \norm{\vw</em>{T-1} + y_{n(t)}\vx_{n(t)}}^2 \
&= \norm{\vw_{T-1}}^2 + 2y_{n(t)}\vw_{T-1}^T\vx_{n(t)} + \norm{y_{n(t)}\vx_{n(t)}}^2\
&\le \norm{\vw_{T-1}}^2 + \norm{y_{n(t)}\vx_{n(t)}}^2\
&= \norm{\vw_{T-1}}^2 + \norm{\vx_{n(t)}}^2\
&\le \norm{\vw_{T-1}}^2 + \max_n\norm{\vx_n}^2\
\end{align*}
$$
With this bound, suppose our $\vw_0$ is the zero vector, then
$$
\norm{\vw_T}^2 \le \norm{\vw_0}^2 + TR^2 = TR^2
$$
where $R^2 = \max_n \norm{\vx_n}^2$ (Consider $R$ as the radius of the dataset).
Let $\rho = \min_n y_n \frac{\vw_f^T}{\norm{\vw_f}}\vx_n$ (closet distance between $\vx$ and $\vw_f$, or margin)
Hence, the normalized inner product has the following property:
$$
\begin{align*}
\frac{\vw^T_f}{\norm{\vw_f}}\frac{\vw_T}{\norm{\vw_T}} &\ge \frac{\vw^T_f}{\norm{\vw_f}}\frac{\vw_T}{\sqrt{TR^2}}\
&= \frac{\vw^T_f}{\norm{\vw_f}}\frac{\vw_T}{\sqrt{T}R}\
&= \frac{1}{\sqrt{T}R}\frac{\vw^T_f}{\norm{\vw_f}}\vw_T\
&= \frac{1}{\sqrt{T}R}\frac{\vw^T_f}{\norm{\vw_f}}(\vw_0 + y_{n(0)}\vx_{n(0)} + y_{n(1)}\vx_{n(1)} + \cdots + y_{n(T-1)}\vx_{n(T-1)})\
&= \frac{1}{\sqrt{T}R}\frac{\vw^T_f}{\norm{\vw_f}}(y_{n(0)}\vx_{n(0)} + y_{n(1)}\vx_{n(1)} + \cdots + y_{n(T-1)}\vx_{n(T-1)})\
&\ge \frac{1}{\sqrt{T}R}T\rho\
&= \frac{\sqrt{T}\rho}{R}.\
\end{align*}
$$
When normalized $\vw_T$ is equivalent as the normalized $\vw_f$, their inner product would be 1, So,
$$
1 \ge \frac{\sqrt{T}\rho}{R}.
$$
Hence,
$$
\sqrt{T} \le \frac{R}{\rho} \Longleftrightarrow T \le \frac{R^2}{\rho^2}.
$$
This means that the number of iterations we find $\vw_f$ is bounded by $R^2 /\rho^2$, which depends on the dataset and $\vw_f$ itself only. So, as long as the dataset is linear separable, PLA halts.</p><h2 id=pros-and-cons-of-pla>Pros and Cons of PLA</h2><p>Pros: It is simple to implement, fast, and works in any dimension $d$.
Cons: We assume $\mathcal{D}$ is linear separable. But, in reality, this property is unknown in advance. Moreover, we are not fully sure how long does the halting take. This is because $\rho$ depends on $\vw_f$, which we never have access to in the first place. Therefore, we run the PLA and observe that it does not halt for a long time. We can&rsquo;t decide between if $\mathcal{D}$ is not linear separable, or if the algorithm is talking a long time to halt.</p><h2 id=learning-with-noisy-data>Learning with Noisy Data</h2><p>Taking a step back, we used to assume that $\cal{Y}$ is generated by applying the target function $f$ on $\mathcal{X}$. However, in reality, there might be noises. So that $\cal{Y} = f(\cal{X}) + \text{noises}$. This means that noisy $\mathcal{D}$ could not linear separable, even though $\cal{D}$ is. How do we learn a $\vw$ in this case?</p><p>First, we assume that the noise is little (which is reasonable), i.e.
$$
y_n = f(\vx_n), \quad \text{usually}.
$$
Therefore, if $g \approx f$, then
$$
y_n = g(\vx_n), \quad \text{usually}.
$$
We want to find a line (hyperplane) $\vw_g$ such that the number of mistakes made on $\mathcal{D}$ is minimized:
$$
\vw_g \leftarrow \arg \min_\vw \sum^N_{n=1} \mathbb{1}_{[y_n \ne \sign(\vw^T\vx_n)]}.
$$
Unfortunately, solving the optimization problem above is NP-hard. However, we can modify PLA to get an approximately good $g$.</p><hr><p>Algorithm (Pocket Algorithm):
Initialize pocket weights $\hat{\vw}$
For $t = 0, 1, \ldots$</p><ol><li>Find a (random) mistake of $\vw_t$ called $\vw_{n(t), y_{n(t)}}$</li><li>(Try to) correct the mistake by $\vw_{t + 1} \leftarrow \vw_t + y_{n(t)}\vw_{n(t)}$</li><li>If $\vw_{t + 1}$ makes fewer mistakes than $\hat{\vw}$, replace $\hat{\vw}$ by $\vw_{t + 1}$
Until enough iterations.
Return $\hat{\vw}$ (called $\vw_{POCKET}$) as $g$</li></ol><hr></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#credit-card-approve-problem>Credit Card Approve Problem</a></li><li><a href=#perceptron-hypothesis>Perceptron Hypothesis</a><ul><li><a href=#perceptron-in-mathbbr2>Perceptron in $\mathbb{R}^2$</a></li></ul></li><li><a href=#perceptron-learning-algorithm>Perceptron Learning Algorithm</a></li><li><a href=#linear-separability>Linear Separability</a></li><li><a href=#pros-and-cons-of-pla>Pros and Cons of PLA</a></li><li><a href=#learning-with-noisy-data>Learning with Noisy Data</a></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f" aria-label=Facebook><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&text=Perceptron%20Learning%20Algorithm" aria-label=Twitter><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&title=Perceptron%20Learning%20Algorithm" aria-label=Linkedin><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&is_video=false&description=Perceptron%20Learning%20Algorithm" aria-label=Pinterest><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Perceptron%20Learning%20Algorithm&body=Check out this article: https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f" aria-label=Email><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&title=Perceptron%20Learning%20Algorithm" aria-label=Pocket><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&title=Perceptron%20Learning%20Algorithm" aria-label=reddit><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&name=Perceptron%20Learning%20Algorithm&description=Given%20a%20dataset%20%24%5cmathcal%7bD%7d%20%3d%20%7b%28%5cvec%7bx%7d_1%2c%20y_1%29%2c%20%5ccdots%2c%20%28%5cvec%7bx%7d_N%2c%20y_N%29%7d%24%20and%20a%20hypothesis%20set%20%24%5cmathcal%7bH%7d%24%2c%20our%20learning%20algorithm%20%24%5cmathcal%7bA%7d%24%20tries%20to%20learn%20a%20function%20%24g%20%5cin%20%5cmathcal%7bH%7d%24%20that%20approximates%20the%20underlying%2c%20true%20function%20%24f%3a%20%5cmathcal%7bX%7d%20%5cto%20%5cmathcal%7bY%7d%24%2c%20which%20generates%20the%20points%20in%20%24%5cmathcal%7bD%7d%24.%0aCredit%20Card%20Approve%20Problem%20Given%20a%20customer%20who%20is%20applying%20for%20a%20credit%20card%2c%20we%20want%20to%20build%20a%20system%20that%20determines%20if%20we%20should%20grant%20the%20application%20or%20not%20based%20on%20the%20customer%26rsquo%3bs%20information%20such%20as%20age%2c%20annual%20salary%2c%20year%20in%20job%2c%20etc." aria-label=Tumblr><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fwww.reidy.icu%2fposts%2fperceptron-learning-algorithm%2f&t=Perceptron%20Learning%20Algorithm" aria-label="Hacker News"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2022 Reid Chen</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/about>About</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
<script src=/js/code-copy.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>