#+TITLE: Matrix Methods in Machine Learning
#+AUTHOR: Yi Chen
#+DATE: <2020-12-15 Tue>
#+tags[]: ML
#+keywords[]: CS ML AI
#+category: notes

#+LATEX_HEADER_EXTRA: \usepackage{mathtools}
#+LATEX_HEADER_EXTRA: \DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
* Unit 2
** Linear Independence and Rank in Learning
*** Linear independence
    A set of $M$ vectors $v_1, v_2, \cdots, v_m \in \mathbb{R}^N$ is linear independent if and only if \[\sum^M_{i=1} v_i\alpha_i = 0 \Leftrightarrow \alpha_i = 0, i = 1, 2, \cdots M.\] Otherwise, it's called linearly dependent.
    A matrix multiplication of $Aw = d$ can be interpreted as the linear combination of the columns of $A$, where the weight for the $i^\text{th}$ column is the $i^\text{th}$ element of $w$.
    So, suppose $A$ is a $3 \times 2$ matrix where the first column is denoted as $a_1$ and the second column is denoted as $a_2$. Then, we can express $Aw = d$ as $a_1w_1 + a_2w_2 = d$.
    This $d$ lies in the span of $A$, which includes all vectors that can be written as $\sum^M_{i=1} a_iw_i$. However, the true value, the value we wish $Aw$ to be, may not lie in the span of $A$. We denote the true value as $\hat{d}$. In the case that $\hat{d}$ does not lie in the span of $A$, $\hat{d} \neq a_1w_1 + a_2w_2$.
*** Rank of a matrix
    The rank of a matrix is the number of linearly independent columns (or rows) that a matrix has. It should be noted that the row rank is equal to the column rank of a matrix. $Rank(A) = n$ means column/row vectors of the matrix can only describe a $n$ dimensional space.
    There is a relationship between rank of a matrix and solution to a linear system:
    1. If $d$ is a linear combination of $a_1, a_2, \cdots, a_m$, then there is a solution, or $rank(A) = rank([A|d]) \Leftrightarrow \text{there is a solution}$
       - So far, it is not clear if there is a solution or infinity many solution
    2. If $a_1, a_2, \cdots, a_m, d$ are linearly independent, then there is no solution, or $rank(A) < rank([A|d]) \Leftrightarrow \text{no solution}$
*** Solutions to Learning Problems
    It is essential to solve $Aw = d$, in many machine learning problems. Such a solution exists only when $d$ and columns vectors of $A$ are linearly dependent, or, $rank(A) = rank([A|d])$. However, there may be more than one solution (infinity many), which happens if and only if the columns of $A$ are linearly dependent. Hence, the relationship between rank of a matrix and solution to a linear system can be extended to:
    1. $rank(A) = rank([A|d]) \Leftrightarrow \text{there is a solution}$
       1. $rank(A) = dim(w) \Leftrightarrow \text{unique solution}$
       2. $rank(A) < dim(w) \Leftrightarrow \text{infinity many solutions}$
    2. $rank(A) < rank([A|d]) \Leftrightarrow \text{no solution}$
    One way to understand that $rank(A) < dim(w)$ implies infinity many solutions is that imagine you have 2 linear equations but 3 variables. One of the variable must be a free one, meaning its value depends on how you set the other two. Hence, there are infinity many solutions.
** Norm
   A norm measures the size of a vector. For example, in the least square problem, we want to find minimize the squared size of the vector $Ax - b$. There are many types of norms, 1-norm, 2-norm, $\infty-\text{norm}$, etc. All of these norms can be generalized as a $p-\text{norm}$:
   \[\norm{x}_p = (\sum_{i=1}^n |x_i|^p)^{\frac{1}{p}}.\]
   So a 2-norm, or the Euclidean norm, is defined as:
   \[\norm{x}_2 = \sqrt{\sum^n_{i=1} x_iw^2}.\]

* Unit 3
** Singular Value Decomposition
   Singular value decomposition (SVD) is a matrix decomposition method that leads to a good low rank approximations. It has a large range of applications.
*** Definition
    Every matrix $A \in \mathbb{R}^{N \times M}$ can be factored into
    \[A = U \Sigma V^T\]
    where $U \in \mathbb{R} ^ {N \times N}$ is an orthogonal matrix, $\Sigma \in \mathbb{R} ^ {N \times M}$ is a diagonal matrix, and $V^T \in \mathbb{R} ^ {M \times M}$ is also an orthogonal matrix.
    Columns of $U$ are called the left singular vectors of $A$; rows of $V^T$, or columns of $V$ are called the right singular vectors of $A$.
    Values on the main diagonal of $\Sigma$ are called singular values, denoted as $\sigma_i$, where $i \in [1, min(M, N)}]$. A property of singular values is that
    \[ \sigma_i \geq \sigma_j \geq 0, \forall i < j.\]
*** Skinny SVD
    If $A \in \mathbb{R}^{N \times M}, N > M$, then $\Sigma \in \mathbb{R} ^ {N \times M}$.
    Since $N > M$, there are $M$ singular values $\sigma$. The rest of the entries in $\Sigma$ are zero. This way, $\Sigma$ can be partitioned into two matrices:
    1. row $1$ to row $M$
    2. row $M + 1$ to row $N$
    Since the the second part is a zero matrix, the result of $U\Sigma$ is determined by the first $M$ columns of $U$ and the first part of $\Sigma$. Hence, we can shrink the singular value decomposition of $A$ into:
    A = U \Sigma V^T \quad s.t. \quad U \in \mathbb{R} ^ {N \times M}, \Sigma \in \mathbb{R} ^ {M \times M}, V^T \in \mathbb{R} ^ {M \times M}.
    This SVD is called the skinny, or economic SVD of $A$.
    Similarly, if $N < M$, then the skinny SVD of $A$ is
    \[A = U \Sigma V^T \quad s.t. \quad U \in \mathbb{R} ^ {M \times M}, \Sigma \in \mathbb{R} ^ {N \times N}, V^T \in \mathbb{R} ^ {N \times M}.\]
*** Low Rank Approximation with SVD
    Given a SVD of a matrix $A$, $A$ can also be written as the sum of outer products of singular vectors:
    \[A = \sum^{M}_{i = 1} \sigma_iu_iv_i^T.\]
    Each $\sigma_iu_iv_i^T$ here is an rank-1 approximation of the orginal matrix $A$. $u_i$ could be seen as a combination of columns of A, $v^T_i$ could be seen as a combination of rows of A, and $\sigma_i$ indicates the importance of $u_iv^T_i$.
    Since, natually, $\sigma_i$ are given in order, $\sigma_1u_1v_1^T$ is the best rank-1 approximiation.
    According to Eckart-Young Theorem, the best rank $k$ approximation of matrix $A$, whose rank is $r > k$, is $\sum^k_{i = 1} \sigma_i u_i v_i^T$.
    - Forbenius norm :: $\norm{A}^2_F = \sum^N_{i=1}\sum^M_{j=1} A_{ij}^2 = \norm{vec(A)}^2_2$
*** SVD Describes Matrix as an Operator
    Given $A \in \mathbb{R} ^ {N \times M}, x \in \mathbb{R}^M, y \in \mathbb{R}^N$. Let
    y = Ax = U\SigmaV^Tx = U[\Sigma (V^Tx)].
    $V^Tx$ can be seen as a rotation of operation of $x$.
    $\Sigma[V^Tx]$ scales $V^Tx$. Finally, $U$ rotates and raises the dimension of $\Sigma V^Tx$ into $\mathbb{R}^N$.
    - Operator norm :: $\norm{A}_2 = \norm{A}_{op} = max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} = \sigma_1$
* Unit 6
** Kernel Regression
   - Why use higher-dimensional feature spaces
   - Reformulate regression in terms of kernels
   - Popular kernels
   - Cautions and considerations
*** Higher dimensional feature spaces extend regression
    $d(x) = w_1x$ can only fit a straight line that passes through the origin.
    However, if we consider $d(x) = w_3x^3 + w_2x^2 + w_1x + w_0$, then a much complex curve can be modeled.
    Let $x = [x_1 \quad x_2 \quad \cdots \quad x_M]^T \in \mathbb{R}^M$.
    Consider $d(x) = \phi^T (x) w$ where $\phi (x) \in \mathbb{R}^P, P > M$.
    For example,
    \[
    x = [x_1 \quad x_2]^T, \phi^T (x) = [x_1^2 \quad x_2^2 \quad \sqrt{2}x_1x_2 \quad x_1 \quad x_2 \quad 1]
    \]
    Finding w using "training" data $x^i, d^i, i = 1, 2, \cdots, N$ with
    \[
    \min_w \sum^N_{i=1} (d^i - \phi^T (x^i) w)^2 + \lambda \norm{w}^2_2.
    \]
    We define $d = [d^1 \quad d^2 \quad \cdots \quad d^N]^T$ and $\Phi = [\phi(x^1) \quad \phi(x^2) \quad \cdots \quad \phi(x^N)]^T$
    We can obtain $w$ by solving the ridge regression problem, whose closed form solution is
    \[
    w = (\Phi^T \Phi + \lambda I)^{-1} \Phi^T d.
    \]
*** Regression is a weight sum of "kernels"
    $d(x) = \phi^T (x) w = \phi^T (x) (\Phi^T \Phi + \lambda I)^{-1} \Phi^T d$.
    Note that $(\Phi^T \Phi + \lambda I)^{-1}$ is of size $P \times P$.
    By the matrix identity: $(\Phi^T \Phi + \lambda I)^{-1}\Phi^T = \Phi^T(\Phi\Phi^T + \lambda I)^{-1}$,
    $d(x) = \phi^T (x) \Phi^T (\Phi\Phi^T + \lambda I)^{-1} d$, with \Phi^T (\Phi\Phi^T + \lambda I)^{-1} of size $N \times N$. \\
    Note that
    \begin{align*}
    [\Phi \Phi ^ T]_{i, j} &= \phi^T (x^i) \phi (x^j) \\
    [\phi^T (x) \Phi ^ T]_{j} &= \phi^T (x) \phi (x^j).
    \end{align*}
    These two equations can be defined to be a kernel
    \begin{align*}
    K(u, v) = \phi^T (u) \phi(v).
    \end{align*}
    If $\alpha = [\alpha_1 \quad \cdots \quad \alpha_N]^T = (\Phi\Phi^T + \lambda I)^{-1} d$, then
    \[
    d(x) = \sum^N_{i=1} \alpha_i \phi^T (x) \phi(x^i) = \sum^N_{i = 1} \alpha_i k(x, x^i).
    \]
*** Kernel methods find $d(x)$ without computing $\phi(x)$
    $d(x) = \sum^N_{i=1} \alpha_i K(x, x^i)$.
    Since $\alpha = (\Phi\Phi^T + \lambda I)^{-1}$, the $\Phi\Phi^T$ can also be represented as a kernel and can be computed efficiently.
*** Popular kernels depend on similarity of $u$, $v$
    Due to the following formula
    \[
    u^Tv = \norm{u}_2\norm{v}_2 cos\theta,
    \]
    the closer $u$ and $v$, the larger the result $u^Tv$.
    - Monomials of degree $q$ :: $K(u, v) = (u^Tv)^q$
    - Polynomials up to degree $q$ :: $K(u, v) = (u^Tv + 1)^q$
    - Gaussian/radial kernel :: $K(u, v) = e^{\frac{-\norm{u - v}^2_2}{2 \sigma^2}}$
    There is no explicit $\phi(x)$ for Gaussian kernel since it has infinity dimensions, meaning it can be applied to all polynomial orders.
    The smoothness of Gaussian kernel is controlled by $\sigma$.
*** Kernel regression considerations
    We should use cross-validation to avoid overfitting with hight dimensional feature spaces.
