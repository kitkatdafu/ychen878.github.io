<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/style.css><link rel=icon type=image/x-icon href=/images/favicon.ico><title>Yi Chen | Ml</title></head><body><div id=content><header><nav><a href=/>[Home]</a>
<a href=/notes/>[Notes]</a>
<a href=/publications/>[Publications]</a>
<a href=/projects/>[Projects]</a><br><a href=https://github.com/kitkatdafu>[GitHub]</a>
<a href=https://archive.casouri.cc>[BHL0388]</a></nav></header><h1>Ml</h1><p><h3><a href=/notes/nuclear-norm-sdp/>Nuclear Norm via SDP</a></h3><i></i>
<time datetime=2023-03-02>Mar 2, 2023</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/nuclear-norm-sdp/><p><p>:PROPERTIES:
:CUSTOM_ID: matrix-norm
:END:</p><h1 id=matrix-norms>Matrix norms</h1><p>Given a matrix $X \in \mathbb{R}^{m \times n}$, $\sigma_{i}(X)$ denotes the $i$-th largest singular value of $X$ and is equal to the square root of the $i$-th largest eigenvalue of $XX&rsquo;$. The rank of $X$, denoted as $\mathrm{rank}(X) = r$ is the number of non-zero singular values.</p><h2 id=inner-product>Inner Product</h2><p>Given $X, Y \in \mathbb{R}^{m \times n}$, the inner product between $X$ and $Y$, denoted by $\langle X, Y\rangle$, is defined as
$$
\langle X, Y \rangle := \mathrm{Tr}(X&rsquo;Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij}Y_{ij} = \mathrm{Tr}(Y&rsquo;X).
$$</p></p></a></p><p><h3><a href=/notes/pla/>Perceptron Learning Algorithm</a></h3><i></i>
<time datetime=2022-11-30>Nov 30, 2022</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/pla/><p><p>Given a dataset
\(\mathcal{D} = \{(\vec{x}_1, y_1), \cdots, (\vec{x}_N, y_N)\}\) and a
hypothesis set \(\mathcal{H}\), our learning algorithm \(\mathcal{A}\)
tries to learn a function \(g \in \mathcal{H}\) that approximates the
underlying, true function \(f: \mathcal{X} \to \mathcal{Y}\), which
generates the points in \(\mathcal{D}\).</p><div id=outline-container-credit-card-approve-problem class=outline-3><h3 id=credit-card-approve-problem>Credit Card Approve Problem</h3><div id=outline-text-credit-card-approve-problem class=outline-text-3><p>Given a customer who is applying for a credit card, we want to build a
system that determines if we should grant the application or not based
on the customer's information such as age, annual salary, year in job,
etc. The bank's historical credit approval data can be seen as a dataset
\(\mathcal{D} = \{(\vec{x}_i, y_i)\}_{i=1}^N\)where each
\(\vx_i \in \mathcal{X}\) and each represents a customer. There is a
target function \(f: \mathcal{X} \to \mathcal{Y}\) that gives \(\vx\)'s
credit behavior \(f(\vec{x}) = y\). Each \(\vx\) is a multidimensional
vector where each component is a feature (age, for example). Our
learning algorithm \(\mathcal{A}\) considers a hypothesis class
\(\mathcal{H}\) and takes the dataset \(\mathcal{D}\) and tries to give
a function \(g \in \mathcal{H}\) so that \(g\) performs similar to
\(f\). We will use \(g\) as our system of approving credit card.</p></p></a></p><p><h3><a href=/notes/clustering/>Clustering</a></h3><i></i>
<time datetime=2022-10-08>Oct 8, 2022</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/clustering/><p><p>In unsupervised learning, there are no labels associated with features.
Generally speaking, the ultimate goal of unsupervised learning is to
find patterns and structures that help us to better understand data.
Sometimes, we also use unsupervised learning to model a distribution.
But we generally will not make predictions.</p><p>There are 3 types of clustering 1. Partitional (centroid,
graph-theoretic, spectral) 1. Hierarchical (agglomerative, divisive) 2.
Bayesian (decision-based, non-parametric)</p><div id=outline-container-partitional-clustering class=outline-3><h3 id=partitional-clustering>Partitional Clustering</h3><div id=outline-text-partitional-clustering class=outline-text-3><div id=outline-container-k-means class=outline-4><h4 id=k-means>\(k\)-means</h4><div id=outline-text-k-means class=outline-text-4><p>\(k\)-means is a type of partitional centroid-based clustering
algorithm. The algorithm is described as follows: 1. Randomly pick \(k\)
cluster centers; 2. Find the closest center for each point; 3. Update
cluster centers by computing centroids; 4. While not converging, jump to
step 2.</p></p></a></p><p><h3><a href=/notes/k-means/>K-means in Python</a></h3><i></i>
<time datetime=2021-10-19>Oct 19, 2021</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/k-means/><p><p>There are two major steps in the K-means algorithm. The first one is to
calculate the representatives (centroids) of a given partition. The
second one is to find the partition based on the representatives.</p><div id=outline-container-inputs class=outline-3><h3 id=inputs>Inputs</h3><div id=outline-text-inputs class=outline-text-3><p>Suppose we have a dataset looks like this:</p><div class="src src-python"><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dataset <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>5</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>]])</span></span></code></pre></div></div><p>Each row in this dataset matrix is an observation and each column in
this matrix represents a feature. So, in this example, we have 5 points
from a plane. And we define partition in the following way:</p></p></a></p></div><footer><p>Copyleft (â†„) 2025 Yi Chen</p><p><a href="mailto: yi.chen@wisc.edu">yi.chen@wisc.edu</a></p></footer><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],macros:{vec:["\\mathbf{#1}",1],vx:"\\vec{x}",vw:"\\vec{w}",sign:"\\textrm{sign}",norm:["\\left\\lVert#1\\right\\rVert",1],st:"\\textrm{subject to:}"}},svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>