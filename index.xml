<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>mrr1vfe</title><link>https://www.reidy.icu/</link><description>Recent content on mrr1vfe</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Reid Chen</copyright><lastBuildDate>Tue, 06 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.reidy.icu/index.xml" rel="self" type="application/rss+xml"/><item><title>Some Useful Lemmas</title><link>https://www.reidy.icu/posts/lemmas/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://www.reidy.icu/posts/lemmas/</guid><description>Completeness 1.1 $-\sup -A = \inf A$ Let $\alpha = \sup -A$. Then, by definition, $\alpha \ge -a, \forall a \in A$. Therefore, $-\alpha \le a, \forall a \in A$. Hence, $-\alpha$ is a lower bound of $A$. Hence, $-\alpha = - \sup -A \le \inf A$.
Let $\beta = \inf A$. Then, by definition, $\beta \le a, \forall a \in A$. Therefore, $-\beta \ge -a, \forall a \in B$. Hence, $-\beta$ is an upper bound of $-A$.</description></item><item><title>About me</title><link>https://www.reidy.icu/about/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://www.reidy.icu/about/</guid><description>I am a graduate student in the ECE department at the University of Wisconsin-Madison, advised by Prof. Ramya Korlakai Vinayak.
CV birds5</description></item><item><title>K-means in Python</title><link>https://www.reidy.icu/posts/k-means/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.reidy.icu/posts/k-means/</guid><description>K-means There are two major steps in the K-means algorithm. The first one is to calculate the representatives (centroids) of a given partition. The second one is to find the partition based on the representatives.
Inputs Suppose we have a dataset looks like this:
1dataset = np.array([[5, 6], 2 [6, 5], 3 [0, 1], 4 [1, 0], 5 [3, 3]]) Each row in this dataset matrix is an observation and each column in this matrix represents a feature.</description></item><item><title/><link>https://www.reidy.icu/posts/clustering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.reidy.icu/posts/clustering/</guid><description>In unsupervised learning, there are no labels associated with features. Generally speaking, the ultimate goal of unsupervised learning is to find patterns and structures that help us to better understand data. Sometimes, we also use unsupervised learning to model a distribution. But we generally will not make predictions.
There are 3 types of clustering
Partitional (centroid, graph-theoretic, spectral) Hierarchical (agglomerative, divisive) Bayesian (decision-based, non-parametric) Partitional Clustering $k$-means $k$-means is a type of partitional centroid-based clustering algorithm.</description></item><item><title/><link>https://www.reidy.icu/posts/perceptron-learning-algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.reidy.icu/posts/perceptron-learning-algorithm/</guid><description>Given a dataset $\mathcal{D} = {(\vec{x}_1, y_1), \cdots, (\vec{x}_N, y_N)}$ and a hypothesis set $\mathcal{H}$, our learning algorithm $\mathcal{A}$ tries to learn a function $g \in \mathcal{H}$ that approximates the underlying, true function $f: \mathcal{X} \to \mathcal{Y}$, which generates the points in $\mathcal{D}$.
Credit Card Approve Problem Given a customer who is applying for a credit card, we want to build a system that determines if we should grant the application or not based on the customer&amp;rsquo;s information such as age, annual salary, year in job, etc.</description></item></channel></rss>