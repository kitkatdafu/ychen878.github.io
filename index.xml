<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>mrr1vfe</title><link>https://www.mrr1vfe.io/</link><description>Recent content on mrr1vfe</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Yi Chen</copyright><lastBuildDate>Sat, 30 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.mrr1vfe.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Reading List</title><link>https://www.mrr1vfe.io/posts/readings/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://www.mrr1vfe.io/posts/readings/</guid><description>Calculus Spivak, Michael. &amp;#34;Calculus, Publish or Perish.&amp;#34; Inc., Houston, Texas (1994). Functions Graphs Matrix Methods Eld√©n, Lars. Matrix methods in data mining and pattern recognition. Society for Industrial and Applied Mathematics, 2007. Vectors and Matrices Linear Systems and Least Squares</description></item><item><title>About me</title><link>https://www.mrr1vfe.io/about/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://www.mrr1vfe.io/about/</guid><description>I am a graduate student in the ECE department at the University of Wisconsin-Madison, advised by Prof. Ramya Korlakai Vinayak.
CV</description></item><item><title>K-means in Python</title><link>https://www.mrr1vfe.io/posts/k-means/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.mrr1vfe.io/posts/k-means/</guid><description>K-means There are two major steps in the K-means algorithm. The first one is to calculate the representatives (centroids) of a given partition. The second one is to find the partition based on the representatives.
Inputs Suppose we have a dataset looks like this:
1dataset = np.array([[5, 6], 2 [6, 5], 3 [0, 1], 4 [1, 0], 5 [3, 3]]) Each row in this dataset matrix is an observation and each column in this matrix represents a feature.</description></item><item><title>Bayesian Statistics</title><link>https://www.mrr1vfe.io/posts/bayesian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.mrr1vfe.io/posts/bayesian/</guid><description>Building a model Designing a simple Bayesian model benefits from a design loop with 3 steps:
Data story: Motivate the model by narrating how the data might arise. Update: Educate your model by feeding it the data. Evaluate: All statistical models require supervision, leading to model revision. Data story Bayesian data analysis usually means producing a story for how the data came to be. Such a story may be descriptive, i.</description></item><item><title>MCMC</title><link>https://www.mrr1vfe.io/posts/mcmc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.mrr1vfe.io/posts/mcmc/</guid><description>About MCMC The esitmation of posterior probability distributions using a stochastic process known as Markov chain Monte Carlo (MCMC).
We will be able to sample directly from the posterior. MCMC requires more computation (takes longer to complete estimation). Metropolis algorithm The metropolis algorihtm works whenever the probability of proposing a jump to $B$ from $A$ is equal to the probability of proposing $A$ from $B$, when the proposal distribution is symmetric.</description></item></channel></rss>