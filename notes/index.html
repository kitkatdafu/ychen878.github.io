<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/style.css><link rel=icon type=image/x-icon href=/images/favicon.ico><title>Yi Chen | Notes</title></head><body><div id=content><header><nav><a href=/>[Home]</a>
<a href=/notes/>[Notes]</a>
<a href=/publications/>[Publications]</a>
<a href=/projects/>[Projects]</a><br><a href=https://github.com/kitkatdafu>[GitHub]</a>
<a href=https://archive.casouri.cc>[BHL0388]</a></nav></header><h1>Notes</h1><p><h3><a href=/notes/nuclear-norm-sdp/>Nuclear Norm via SDP</a></h3><i></i>
<time datetime=2023-03-02>Mar 2, 2023</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/nuclear-norm-sdp/><p>:PROPERTIES: :CUSTOM_ID: matrix-norm :END:
Matrix norms Given a matrix $X \in \mathbb{R}^{m \times n}$, $\sigma_{i}(X)$ denotes the $i$-th largest singular value of $X$ and is equal to the square root of the $i$-th largest eigenvalue of $XX&rsquo;$. The rank of $X$, denoted as $\mathrm{rank}(X) = r$ is the number of non-zero singular values.
Inner Product Given $X, Y \in \mathbb{R}^{m \times n}$, the inner product between $X$ and $Y$, denoted by $\langle X, Y\rangle$, is defined as $$ \langle X, Y \rangle := \mathrm{Tr}(X&rsquo;Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij}Y_{ij} = \mathrm{Tr}(Y&rsquo;X).</p></a></p><p><h3><a href=/notes/lp-duality/>LP Duality</a></h3><i></i>
<time datetime=2023-02-28>Feb 28, 2023</time>
<i></i>
<a href=https://ychen878.github.io/tags/>[optimization]</a>
<a href=/notes/lp-duality/><p>Estimating LP bounds Given an optimization problem $$ \begin{align} \max_{f, s} &\quad 12f + 9s \ \st &\quad 4f + 2s \leq 4800 \ &\quad f + s \leq 1750 \ &\quad 0 \leq f \leq 1000 \ &\quad 0 \leq s \leq 1500 \ \end{align} $$ Suppose the maximum profit is $p^\star$. How can we bound $p^\star$? The lower bound of $p^\star$ can be found by picking any feasible point (since maximization).</p></a></p><p><h3><a href=/notes/rpc/>RPC</a></h3><i></i>
<time datetime=2023-02-25>Feb 25, 2023</time>
<i></i>
<a href=https://ychen878.github.io/tags/>[big data]</a>
<a href=/notes/rpc/><p>Networks Network Interface Controllers (NICs) can connect a computer to different physical mediums, such as Ethernet and Wi-Fi. Every NIC in the world has a unique MAC (media access control) address. It consists of 48 bits. Therefore, there are 28 trillion possible MAC addresses. Some devices randomly change their MAC address for privacy.
You can use command ifconfig to check you network interface controller and its corresponding MAC address. There exists virtual interfaces as well.</p></a></p><p><h3><a href=/notes/docker/>Docker</a></h3><i></i>
<time datetime=2023-02-25>Feb 25, 2023</time>
<i></i>
<a href=https://ychen878.github.io/tags/>[big data]</a>
<a href=/notes/docker/><p>Virtualization Virtualization is the illusion of private resources, provided by the software. We have virtual memory, virtual machine (hardware), virtual machine (languages), virtual operating system (container).
Each process using a virtual address space is not aware of other processes using memory (illusion of private memory). Virtualized resources include CPU, RAM, disks, network devices, etc. VMs rarely use all their allocated resources, so overbooking is possible. If each program is deployed to a different VM, operating system overheads dominate.</p></a></p><p><h3><a href=/notes/pla/>Perceptron Learning Algorithm</a></h3><i></i>
<time datetime=2022-11-30>Nov 30, 2022</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/pla/><p>Given a dataset \(\mathcal{D} = \{(\vec{x}_1, y_1), \cdots, (\vec{x}_N, y_N)\}\) and a hypothesis set \(\mathcal{H}\), our learning algorithm \(\mathcal{A}\) tries to learn a function \(g \in \mathcal{H}\) that approximates the underlying, true function \(f: \mathcal{X} \to \mathcal{Y}\), which generates the points in \(\mathcal{D}\).
Credit Card Approve Problem Given a customer who is applying for a credit card, we want to build a system that determines if we should grant the application or not based on the customer's information such as age, annual salary, year in job, etc.</p></a></p><p><h3><a href=/notes/clustering/>Clustering</a></h3><i></i>
<time datetime=2022-10-08>Oct 8, 2022</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/clustering/><p>In unsupervised learning, there are no labels associated with features. Generally speaking, the ultimate goal of unsupervised learning is to find patterns and structures that help us to better understand data. Sometimes, we also use unsupervised learning to model a distribution. But we generally will not make predictions.
There are 3 types of clustering 1. Partitional (centroid, graph-theoretic, spectral) 1. Hierarchical (agglomerative, divisive) 2. Bayesian (decision-based, non-parametric)
Partitional Clustering \(k\)-means \(k\)-means is a type of partitional centroid-based clustering algorithm.</p></a></p><p><h3><a href=/notes/k-means/>K-means in Python</a></h3><i></i>
<time datetime=2021-10-19>Oct 19, 2021</time>
<i></i>
<a href=https://ychen878.github.io/tags/>ml</a>
<a href=/notes/k-means/><p>There are two major steps in the K-means algorithm. The first one is to calculate the representatives (centroids) of a given partition. The second one is to find the partition based on the representatives.
Inputs Suppose we have a dataset looks like this:
dataset = np.array([[5, 6], [6, 5], [0, 1], [1, 0], [3, 3]]) Each row in this dataset matrix is an observation and each column in this matrix represents a feature.</p></a></p></div><footer><p>Copyleft (â†„) 2024 Yi Chen</p><p><a href="mailto: yi.chen@wisc.edu">yi.chen@wisc.edu</a></p></footer><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],macros:{vec:["\\mathbf{#1}",1],vx:"\\vec{x}",vw:"\\vec{w}",sign:"\\textrm{sign}",norm:[`\\left\\lVert#1\\right\\rVert`,1],st:"\\textrm{subject to:}"}},svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>